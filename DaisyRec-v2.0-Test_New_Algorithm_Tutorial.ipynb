{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Alt Text\" width=\"700\">\n",
    "\n",
    "\n",
    "# Daisy Rec Evaluate New Algorithm tutorial\n",
    "\n",
    "This is a tutorial under construction on how to add a new model into DaisyRec and evaluate its test metrics. \n",
    "This tutorial is still under construction so please do report on any bugs you might find!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps \n",
    "\n",
    " Say you want to re-implement the Neural Collaborative Filtering algorithm as designed by He et al. (2017) (archiv link: [arxiv.org/abs/1708.05031](https://arxiv.org/abs/1708.05031)). The following is a guide on implementation  \n",
    " \n",
    " First, **create a shortened name string**. For this implementation, we use \"neumf\". Remember this string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1 - adding default hyperparameter configurations\n",
    " \n",
    "1. Go to folder 'daisy/assets'\n",
    "\n",
    "2. Create a yaml config file for the model using the name. This case the new file would be 'daisy/assets/neumf.yaml'\n",
    "\n",
    "3. Inside the YAML file, input **all** the hyperparameters in as keys yaml format. The values will be the default values. In this case, we have:\n",
    "\n",
    "```\n",
    "    # Hyperparameters\n",
    "    factors: 24\n",
    "    num_layers: 2\n",
    "    dropout: 0.5\n",
    "    lr: 0.001\n",
    "    epochs: 30\n",
    "    reg_1: 0.001\n",
    "    reg_2: 0.001\n",
    "    GMF_model: ~\n",
    "    MLP_model: ~\n",
    "\n",
    "    # Model name\n",
    "    model_name: NeuMF\n",
    "```\n",
    "\n",
    "You may input your custom MLP or GMF model into the .yaml file, or put ~ to use our implementation. Note that these values are indeed **default only**, they can be tuned or different values can be tested using command line arguments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - adding the model .py file\n",
    "\n",
    "1. Create the new model python file, following naming convention of adding \"Recommender\" at the back of the model name i.e., NeuMFRecommender.py\n",
    "\n",
    "2. Put the file into the daisy/models folder, in the /accuracy subfolder or /diversity subfolder, depending on if your model is focused on increasing accuracy or diversity of recommendation. In this case, since our model is accuracy-based, the absolute file path of our model code (from the root folder) is /daisy/model/accuracy/NeuMFRecommender.py\n",
    "\n",
    "\n",
    "3. Inside the file, define your model class briefly; we will go into deeper details later. Name your class the model name in your yaml file. Your model should usually be a child class of `GeneralRecommender`, imported from daisy/model/AbstractRecommender.py. Please add a class variable `tunable_param_names` which is an array with all of the hyperparameters as outlined in the yaml file. In this case, it would be:\n",
    "\n",
    "```\n",
    "from daisy.model.AbstractRecommender import GeneralRecommender\n",
    "\n",
    "class NeuMF(GeneralRecommender):\n",
    "    tunable_param_names = ['num_ng', 'factors', 'num_layers', 'dropout', 'lr', 'batch_size', 'reg_1', 'reg_2']\n",
    "    '''\n",
    "    NeuMF Recommender Class, it can be seperate as: GMF and MLP\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__(config)\n",
    "        self.config = config\n",
    "```\n",
    "\n",
    "4. Now, import the Model in daisy/model/Models.py, which is used for importing models into other files. Inside the RecommenderModel() function, you will see a large if-else block matching the model name with the model class import. For this case, we will add the `elif` block:\n",
    "\n",
    "```\n",
    "    elif algo_name == 'neumf':\n",
    "    from daisy.model.accuracyRecommender.NeuMFRecommender import NeuMF\n",
    "    return NeuMF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Loading data into the model\n",
    "\n",
    "The dataset is loaded in test.py and tune.py using:\n",
    "\n",
    "```\n",
    "''' Train Test split '''\n",
    "    splitter = TestSplitter(config)\n",
    "    train_index, test_index = splitter.split(df)\n",
    "    train_set, test_set = df.iloc[train_index, :].copy(), df.iloc[test_index, :].copy()\n",
    "```\n",
    "\n",
    "`train_set` is just a portion of the full data in a Pandas DataFrame with columns corresponding to user IDs, item IDs, ratings and timestamps. The user IDs and item IDs numbering start from 0, not necessarily in order. For example:\n",
    "\n",
    "| User ID | Item ID | Rating | Timestamp |\n",
    "|---------|---------|--------|-----------|\n",
    "|   133 |   2023 |   4.5  |  1624165321  |\n",
    "|   0 |   345 |   3.8  |  1624165487  |\n",
    "|   210 |   293 |   5.0  |  1624165632  |\n",
    "\n",
    "Some datasets have explicit feedback (i.e., ratings are 0.0/5 to 5.0/5) whereas some only have implicit feedback (i.e., only interaction existence is captured. Rating is hard set to 1.0/5). Inspect the dataset you want before use.\n",
    "\n",
    "For negative sampling, we will use `BasicNegtiveSampler`. If you need some special processing, feel free to create your own custom sampler, or explore the functionality of `AEDataset` and `SkipGramNegativeSampler` and see if these classes are performing the processing that you are looking for. \n",
    "\n",
    "Most of the methods (especially neural methods) need to convert into a pytorch DataLoader. In this case, we do:\n",
    "\n",
    "```\n",
    "sampler = BasicNegativeSampler(train_set, config)\n",
    "train_samples = sampler.sampling() # This returns a numpy array or pandas df\n",
    "train_dataset = BasicDataset(train_samples) # Converts pd.df/np.ndarray to simple pytorch Dataset\n",
    "train_loader = get_dataloader(\n",
    "    train_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=4) # Convert torch Dataset to torch DataLoader\n",
    "```\n",
    "\n",
    "Now, `train_loader` is the data loader, which will be the input to `model.fit()` to be explained shortly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - incorporating into tune.py and test.py\n",
    "\n",
    "We now need to put the model name into the model builder in both tune.py and test.py. \n",
    "\n",
    "1. In each file, search \"if config['algo_name'].lower() in\" in your code editor. You should come across the following code block:\n",
    "    \n",
    "    <img src='images\\new_algo\\testpy.png' alt=\"image in test.py\" width=\"700\"/>\n",
    "    \n",
    "\n",
    "2. This code block basically:\n",
    "    - Builds the model that you want using `RecommenderModel()`\n",
    "    - Pre-processes the raw `train_set` data into a format that is your model needs. This includes performing negative sampling  and converting the raw pandas df or numpy array into a torch DataLoader `train_loader`\n",
    "    - Fits your model to the training data using `model.fit(train_loader)`\n",
    "\n",
    "\n",
    "3. Notice that each algorithm shorthand name is in an array corresponding to whatever settings are needed for building and fitting that model. As shown in the blue circle, we add our model name 'neumf' into the array corresponding to the data processing that we need to fit our model. If you need custom processing, feel free to create your own `elif` block "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Understanding `AbstractRecommender` and `GeneralRecommender` \n",
    "\n",
    "Use the `self.config` object for all the configuration/hyperparameter information needed for the model. Use this function to train the model and store this information inside the model object. We will be using `model.rank()` for testing the model later for top-n ranking (or for \"forward-propagation\" of neural methods). Following is an example of how it is done in `SndMostPop.fit()`:\n",
    "\n",
    "```\n",
    "    def fit(self, training_df: DataFrame) -> np.ndarray:\n",
    "        '''\n",
    "        Ranks for item in the training dataframe by its popularity (number of user-item interactions)\n",
    "        returns this 1-D array of ranks\n",
    "        '''\n",
    "        items_column = training_df[self.config['IID_NAME']]  # config['IID_NAME'] usually returns 'item'\n",
    "        self.item_counts_series = items_column.value_counts()\n",
    "\n",
    "        return self.item_counts_series\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Designing the `model.rank()`\n",
    "\n",
    "Unlike `fit()`, rank() does not take in any data type as an argument other than a torch Dataloader; thus, it is important to follow the following function signature:\n",
    "\n",
    "```\n",
    "    def rank(self, test_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "```\n",
    "\n",
    "### The function input: `test_loader`\n",
    "\n",
    "As for the dataset contained in the DataLoader called `test_loader`, remember that these data are contained in batches (default 128). The data is two-dimensional, so we have 128 arrays (rows). Each array is for each user, so the array will contain 1. **the user ID** and 2. **another array of negatively-sampled items, called the candidates set** (i.e. items with no interaction data for which a recommendation needs to be made for the user and ranked).\n",
    "\n",
    "So we will have 128 arrays of:\n",
    "\n",
    "[ user-id, [... 1000 candidate_item_ids ...]]\n",
    "\n",
    "### The function output: `t`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
