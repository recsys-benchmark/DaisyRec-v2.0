{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Alt Text\" width=\"700\">\n",
    "\n",
    "\n",
    "# Daisy Rec Evaluate New Algorithm tutorial\n",
    "\n",
    "This is a tutorial under construction on how to add a new model into DaisyRec and evaluate its test metrics. \n",
    "This tutorial is still under construction so please do report on any bugs you might find!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps \n",
    "\n",
    " Say you want to create a fantastic new recommender algorithm called Second Most Popular, where you take the top-n+1 most popular items and cut off the most popular\n",
    " \n",
    " First, **create a shortened name string** , e.g. \"sndmostpop\". Remember this string.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1 - adding default hyperparameter configurations\n",
    " \n",
    "1. Go to folder 'daisy/assets'\n",
    "\n",
    "2. Create a yaml config file for the model using the name. This case the new file would be 'daisy/assets/sndmostpop.yaml'\n",
    "\n",
    "3. Inside the YAML file, input all the **default** hyperparameters in yaml format. Since this algorithm has no hyperparametrs, the file will be left empty (but still need to be created).\n",
    "\n",
    "![Alt Text](./images/new_algo/new_yaml.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - adding the model .py file\n",
    "\n",
    "1. Create the new model python file, following naming convention of adding \"Recommender\" at the back E.g., SecondMostPopRecommender.py\n",
    "\n",
    "2. Put the file into the daisy/models folder, in the /accuracy subfolder or /diversity subfolder, depending on if your model is focused on increasing accuracy or diversity of recommendation.\n",
    "\n",
    "![Alt Text](./images/new_algo/new_py.png)\n",
    "\n",
    "3. Inside the file, define your model class briefly; we will go into deeper details later. Define a preliminary fit(self) and predict(self) function, ignore the function signatures for now. Your model should be a child class of GeneralRecommender. In this case, it would be:\n",
    "\n",
    "\n",
    "    ```\n",
    "    from daisy.model.AbstractRecommender import GeneralRecommender\n",
    "    from pandas import DataFrame\n",
    "    import torch\n",
    "    \n",
    "    class SndMostPop(GeneralRecommender):\n",
    "        '''\n",
    "        Model recommends user the second most popular items in the list\n",
    "        '''\n",
    "        tunable_param_names = [] # this should be a list of all hyperparameter names\n",
    "        def __init__(self, config):\n",
    "            super(SndMostPop, self).__init__(config)\n",
    "            self.config = config\n",
    "\n",
    "        def fit(self, training_data: DataFrame) -> np.ndarray:\n",
    "            pass\n",
    "        \n",
    "        def rank(self, test_loader: torch.utils.data.DataLoader ) -> np.ndarray:\n",
    "            pass\n",
    "    ```\n",
    "\n",
    "4. Now, import the Model in daisy/model/Models.py, which is used for importing models into other files:\n",
    "\n",
    "![Alt](./images/new_algo/modelspy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - incorporating into tune.py and test.py\n",
    "\n",
    "We now need to put the model name into the model builder in both tune.py and test.py. \n",
    "\n",
    "1. In each file, search \"if config['algo_name'].lower() in\" in your code editor. You should come across the following code block:\n",
    "\n",
    "    ![image.png](./images/new_algo/testpy.png)\n",
    "\n",
    "2. This code block basically:\n",
    "    - Builds the model that you want\n",
    "    - Pre-processes the raw training data (called `train_set`, in a Pandas DataFrame) into a format that is your model needs\n",
    "    - Fits your model to the training data using model.fit()\n",
    "\n",
    "\n",
    "3. Notice that each algorithm shorthand name is in an array corresponding to whatever settings are needed for building and fitting that model. As shown in the blue circle, add your model name into the array corresponding to the data-type for the training data that you need to fit your model (see step 4 for choosing which one). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Which data type do I need for my model to work?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the code blocks above, they are mainly focusing on taking the `train_set`, pre-processing it and converting it into a data format that the model needs.\n",
    "\n",
    "`train_set` is just a Pandas DataFrame with columns corresponding to user IDs, item IDs, ratings and timestamps. The user IDs and item IDs numbering start from 0, not necessarily in order. For example:\n",
    "\n",
    "| User ID | Item ID | Rating | Timestamp |\n",
    "|---------|---------|--------|-----------|\n",
    "|   1 |   2023 |   4.5  |  1624165321  |\n",
    "|   0 |   4 |   3.8  |  1624165487  |\n",
    "|   2 |   293 |   5.0  |  1624165632  |\n",
    "\n",
    "Some datasets have explicit feedback (i.e., ratings are 0.0/5 to 5.0/5) whereas some only have implicit feedback (i.e., only interaction existence is captured. Rating is hard set to 1.0/5). Inspect the dataset you want before use.\n",
    "\n",
    "If you need some special processing, feel free to explore the functionality of `AEDataset`, `BasicNegtiveSampler` and `SkipGramNegativeSampler` and see if these classes are performing the processing that you are looking for. \n",
    "\n",
    "Most of the methods (especially neural methods) need to convert into a pytorch DataLoader. In this case, we do:\n",
    "\n",
    "```\n",
    "    sampler = MySampler(train_set, config)\n",
    "    train_samples = sampler.sampling() # This returns a numpy array or pandas df\n",
    "    train_dataset = BasicDataset(train_samples) # Converts pd.df/np.ndarray to simple pytorch Dataset\n",
    "    train_loader = get_dataloader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        num_workers=4) # Convert to DataLoader\n",
    "    model.fit(train_loader)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Designing the `model.fit()` \n",
    "\n",
    "Use the `self.config` object for all the configuration/hyperparameter information needed for the model. Use this function to train the model and store this information inside the model object. We will be using `model.rank()` for testing the model later for top-n ranking (or for \"forward-propagation\" of neural methods). Following is an example of how it is done in `SndMostPop.fit()`:\n",
    "\n",
    "```\n",
    "    def fit(self, training_df: DataFrame) -> np.ndarray:\n",
    "        '''\n",
    "        Ranks for item in the training dataframe by its popularity (number of user-item interactions)\n",
    "        returns this 1-D array of ranks\n",
    "        '''\n",
    "        items_column = training_df[self.config['IID_NAME']]  # config['IID_NAME'] usually returns 'item'\n",
    "        self.item_counts_series = items_column.value_counts()\n",
    "\n",
    "        return self.item_counts_series\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Designing the `model.rank()`\n",
    "\n",
    "Unlike `fit()`, rank() does not take in any data type as an argument other than a torch Dataloader; thus, it is important to follow the following function signature:\n",
    "\n",
    "```\n",
    "    def rank(self, test_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "```\n",
    "\n",
    "### The function input: `test_loader`\n",
    "\n",
    "As for the dataset contained in the DataLoader called `test_loader`, remember that these data are contained in batches (default 128). The data is two-dimensional, so we have 128 arrays (rows). Each array is for each user, so the array will contain 1. **the user ID** and 2. **another array of negatively-sampled items, called the candidates set** (i.e. items with no interaction data for which a recommendation needs to be made for the user and ranked).\n",
    "\n",
    "So we will have 128 arrays of:\n",
    "\n",
    "[ user-id, [... 1000 candidate_item_ids ...]]\n",
    "\n",
    "### The function output: `t`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
